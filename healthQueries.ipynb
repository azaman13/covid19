{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from collections import Counter\n",
    "stop_words = set(stopwords.words('english')) \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/Users/miriamblumenthal/Desktop/Desktop/GoogleTake2/search+youtube_v4/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1.1</th>\n",
       "      <th>ratings</th>\n",
       "      <th>action</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>days</th>\n",
       "      <th>hours</th>\n",
       "      <th>minutes</th>\n",
       "      <th>weekdays</th>\n",
       "      <th>link</th>\n",
       "      <th>titles</th>\n",
       "      <th>source</th>\n",
       "      <th>query</th>\n",
       "      <th>datetime</th>\n",
       "      <th>all_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>815</td>\n",
       "      <td>815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Searched</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>pichai net worth</td>\n",
       "      <td>2020-01-07 02:46:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>809</td>\n",
       "      <td>809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Searched</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>josep maria bartomeu floreta net worth</td>\n",
       "      <td>2020-01-07 02:51:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>805</td>\n",
       "      <td>805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Searched</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>richest athlete</td>\n",
       "      <td>2020-01-07 02:55:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>773</td>\n",
       "      <td>773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>What China Is Really Up To In Africa - Forbes</td>\n",
       "      <td>2020-01-07 19:35:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>623</td>\n",
       "      <td>623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Searched</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>kagame election 2017</td>\n",
       "      <td>2020-01-17 19:15:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>3937</td>\n",
       "      <td>3937</td>\n",
       "      <td>3937</td>\n",
       "      <td>3937</td>\n",
       "      <td>928</td>\n",
       "      <td>585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>Section III HS scoreboard for Thursday, Sept. ...</td>\n",
       "      <td>2020-04-29 16:25:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403</th>\n",
       "      <td>3953</td>\n",
       "      <td>3953</td>\n",
       "      <td>3953</td>\n",
       "      <td>3953</td>\n",
       "      <td>912</td>\n",
       "      <td>569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>Obituaries | nny360.com - Watertown Daily Times</td>\n",
       "      <td>2020-04-30 12:01:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>4009</td>\n",
       "      <td>4009</td>\n",
       "      <td>4009</td>\n",
       "      <td>4009</td>\n",
       "      <td>881</td>\n",
       "      <td>538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>Watertown Daily Times</td>\n",
       "      <td>2020-04-30 15:42:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>4237</td>\n",
       "      <td>4237</td>\n",
       "      <td>4237</td>\n",
       "      <td>4237</td>\n",
       "      <td>657</td>\n",
       "      <td>314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>Amid Ongoing COVID-19 Pandemic, Governor Cuomo...</td>\n",
       "      <td>2020-05-10 21:41:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>4573</td>\n",
       "      <td>4573</td>\n",
       "      <td>4573</td>\n",
       "      <td>4573</td>\n",
       "      <td>346</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>search</td>\n",
       "      <td>Wedding: Richard A. Duvall and Elizabeth M. Ma...</td>\n",
       "      <td>2020-05-28 19:03:00</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2407 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 Unnamed: 0.1 Unnamed: 0.1.1 Unnamed: 0.1.1.1  \\\n",
       "0            62           62             62               62   \n",
       "1            67           67             67               67   \n",
       "2            71           71             71               71   \n",
       "3           103          103            103              103   \n",
       "4           253          253            253              253   \n",
       "...         ...          ...            ...              ...   \n",
       "2402       3937         3937           3937             3937   \n",
       "2403       3953         3953           3953             3953   \n",
       "2404       4009         4009           4009             4009   \n",
       "2405       4237         4237           4237             4237   \n",
       "2406       4573         4573           4573             4573   \n",
       "\n",
       "     Unnamed: 0.1.1.1.1 Unnamed: 0.1.1.1.1.1 ratings    action dislikes likes  \\\n",
       "0                   815                  815     NaN  Searched      NaN   NaN   \n",
       "1                   809                  809     NaN  Searched      NaN   NaN   \n",
       "2                   805                  805     NaN  Searched      NaN   NaN   \n",
       "3                   773                  773     NaN   Visited      NaN   NaN   \n",
       "4                   623                  623     NaN  Searched      NaN   NaN   \n",
       "...                 ...                  ...     ...       ...      ...   ...   \n",
       "2402                928                  585     NaN   Visited      NaN   NaN   \n",
       "2403                912                  569     NaN   Visited      NaN   NaN   \n",
       "2404                881                  538     NaN   Visited      NaN   NaN   \n",
       "2405                657                  314     NaN   Visited      NaN   NaN   \n",
       "2406                346                    3     NaN   Visited      NaN   NaN   \n",
       "\n",
       "      ... days hours minutes weekdays link titles  source  \\\n",
       "0     ...    7     2      46        1  NaN    NaN  search   \n",
       "1     ...    7     2      51        1  NaN    NaN  search   \n",
       "2     ...    7     2      55        1  NaN    NaN  search   \n",
       "3     ...    7    19      35        1  NaN    NaN  search   \n",
       "4     ...   17    19      15        4  NaN    NaN  search   \n",
       "...   ...  ...   ...     ...      ...  ...    ...     ...   \n",
       "2402  ...   29    16      25        2  NaN    NaN  search   \n",
       "2403  ...   30    12       1        3  NaN    NaN  search   \n",
       "2404  ...   30    15      42        3  NaN    NaN  search   \n",
       "2405  ...   10    21      41        6  NaN    NaN  search   \n",
       "2406  ...   28    19       3        3  NaN    NaN  search   \n",
       "\n",
       "                                                  query             datetime  \\\n",
       "0                                      pichai net worth  2020-01-07 02:46:00   \n",
       "1                josep maria bartomeu floreta net worth  2020-01-07 02:51:00   \n",
       "2                                       richest athlete  2020-01-07 02:55:00   \n",
       "3         What China Is Really Up To In Africa - Forbes  2020-01-07 19:35:00   \n",
       "4                                  kagame election 2017  2020-01-17 19:15:00   \n",
       "...                                                 ...                  ...   \n",
       "2402  Section III HS scoreboard for Thursday, Sept. ...  2020-04-29 16:25:00   \n",
       "2403    Obituaries | nny360.com - Watertown Daily Times  2020-04-30 12:01:00   \n",
       "2404                              Watertown Daily Times  2020-04-30 15:42:00   \n",
       "2405  Amid Ongoing COVID-19 Pandemic, Governor Cuomo...  2020-05-10 21:41:00   \n",
       "2406  Wedding: Richard A. Duvall and Elizabeth M. Ma...  2020-05-28 19:03:00   \n",
       "\n",
       "     all_categories  \n",
       "0              News  \n",
       "1              News  \n",
       "2              News  \n",
       "3              News  \n",
       "4              News  \n",
       "...             ...  \n",
       "2402           News  \n",
       "2403           News  \n",
       "2404           News  \n",
       "2405           News  \n",
       "2406           News  \n",
       "\n",
       "[2407 rows x 24 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news = df_science = df_jobs = df_business = df_health = pd.DataFrame(columns=pd.read_csv(files[0]).columns)\n",
    "for f in files:\n",
    "#     df = pd.read_csv(f)\n",
    "#     df_jan = df_jan.append(df[(df.years == 2020) & (df.months == 1)], ignore_index=True)\n",
    "#     df_feb = df_feb.append(df[(df.years == 2020) & (df.months == 2)], ignore_index=True)\n",
    "#     df_march = df_march.append(df[(df.years == 2020) & (df.months == 3)], ignore_index=True)\n",
    "#     df_april = df_april.append(df[(df.years == 2020) & (df.months == 4)], ignore_index=True)\n",
    "#     df_may = df_may.append(df[(df.years == 2020) & (df.months == 5)], ignore_index=True)\n",
    "#     df_june = df_june.append(df[(df.years == 2020) & (df.months == 6)], ignore_index=True)\n",
    "    df = pd.read_csv(f)\n",
    "    df_news = df_news.append(df[df['all_categories'] == \"News\"], ignore_index=True)\n",
    "    df_science = df_science.append(df[df['all_categories'] == \"Science\"], ignore_index=True)\n",
    "    df_jobs = df_jobs.append(df[df['all_categories'] == \"Jobs & Education\"], ignore_index=True)\n",
    "    df_business = df_business.append(df[df['all_categories'] == \"Business & Industrial\"], ignore_index=True)\n",
    "    df_health = df_business.append(df[df['all_categories'] == \"Health\"], ignore_index=True)\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df_name):\n",
    "    text = df_name['query'].tolist()\n",
    "    map(word_tokenize, text)\n",
    "    df_name['query'].apply(word_tokenize)\n",
    "    df_name['query'].apply(word_tokenize).tolist()\n",
    "    tag_list = nltk.pos_tag_sents(df_name['query'].apply(word_tokenize).tolist())\n",
    "    return tag_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(tagged_list):\n",
    "    noun_list = []\n",
    "    for i in tagged_list:\n",
    "        temp = [j[0] for j in i if j[1].startswith(\"NN\" or \"NNP\" or \"NNS\" or \"NNPS\")]\n",
    "        noun_list.append(temp)\n",
    "\n",
    "    noun_list = [[x.lower() for x in sublst] for sublst in noun_list]\n",
    "    return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj(tagged_list):\n",
    "    adj_list = []\n",
    "    for i in tagged_list:\n",
    "        temp = [j[0] for j in i if j[1].startswith(\"JJ\" or \"JJR\" or \"JJS\")]\n",
    "        adj_list.append(temp)\n",
    "\n",
    "    adj_list = [[x.lower() for x in sublst] for sublst in adj_list]\n",
    "    return adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(tagged_list):\n",
    "    verb_list = []\n",
    "    for i in tagged_list:\n",
    "        temp = [j[0] for j in i if j[1].startswith(\"VB\" or \"VBD\" or \"VBG\" or \"VBN\" or \"VBP\" or \"VBZ\")]\n",
    "        verb_list.append(temp)\n",
    "\n",
    "    verb_list = [[x.lower() for x in sublst] for sublst in verb_list]\n",
    "    return verb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['bartomeu'],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['|'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['wants', 'send'],\n",
       " [],\n",
       " ['defends'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['breaking'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['won'],\n",
       " [],\n",
       " ['warning', 'issued'],\n",
       " [],\n",
       " ['open'],\n",
       " ['make'],\n",
       " ['did', 'have'],\n",
       " [],\n",
       " ['pardon'],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " ['has'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['is', 'did', 'lie', 'did', 'die'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['is', 'correct'],\n",
       " [],\n",
       " [],\n",
       " ['shit'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['invade', 'kill'],\n",
       " [],\n",
       " [],\n",
       " ['has'],\n",
       " [],\n",
       " [],\n",
       " ['love', 'pick'],\n",
       " ['carpet'],\n",
       " [],\n",
       " ['says', 'believe', 'promotes'],\n",
       " ['|'],\n",
       " [\"d'état\"],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['earn'],\n",
       " [],\n",
       " ['is'],\n",
       " ['is', 'hypocrisy', 'am'],\n",
       " ['is'],\n",
       " ['is'],\n",
       " ['|'],\n",
       " ['is'],\n",
       " ['did', 'obama', 'become'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['ask', 'call'],\n",
       " ['anthem'],\n",
       " ['love', 'see', \"'s\"],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['obama'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['windows', 'updated'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['addresses'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['settle'],\n",
       " ['is', 'has'],\n",
       " ['are'],\n",
       " ['mixigaming'],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['gets'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['kaling'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['hits'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['does', 'deidara', 'hate'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['be', 'made', 'play'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['windows', 'updated'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['addresses'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['oscillate', 'is', 'attached'],\n",
       " ['settle'],\n",
       " ['is', 'has'],\n",
       " ['are'],\n",
       " ['mixigaming'],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['lộ'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['|'],\n",
       " [],\n",
       " [],\n",
       " ['reeves'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['detailed'],\n",
       " [],\n",
       " [],\n",
       " ['|'],\n",
       " ['prepares', 'welcome'],\n",
       " ['came'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['cut'],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " ['speaks'],\n",
       " ['starts'],\n",
       " [],\n",
       " ['breaking'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['provides'],\n",
       " [],\n",
       " [],\n",
       " ['|'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['asks'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['does'],\n",
       " ['considered', 'touch'],\n",
       " ['times', 'touch'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['dave'],\n",
       " ['dave'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['buy', '|'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['responds'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['kylie'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['is', 'rihanna'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['would', \"'ve\", \"'knocked\"],\n",
       " ['is', 'think'],\n",
       " ['reeves'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['did', 'trump', 'go'],\n",
       " ['did', 'ivanka', 'go'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['ramp'],\n",
       " ['is', '—', \"'s\"],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['ted'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['height'],\n",
       " [],\n",
       " ['donald'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " ['does', 'have'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " ['said', 'call'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['told'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['|'],\n",
       " ['learn', 'do'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['breaking'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['lux'],\n",
       " [],\n",
       " ['breaking'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['watch', '|'],\n",
       " [],\n",
       " [],\n",
       " ['do'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['maxwell', 'saga'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['releases'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['need', 'know'],\n",
       " ['is', 'dating'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['|'],\n",
       " [],\n",
       " [],\n",
       " ['is', 'be'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['claims', 'recall'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['prestwich'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['dating'],\n",
       " [],\n",
       " ['try'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " ['nina'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['seeks'],\n",
       " [],\n",
       " ['wants', 'start', 'sending', 'use'],\n",
       " ['vote'],\n",
       " [],\n",
       " [],\n",
       " ['dating'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['isabelle'],\n",
       " [],\n",
       " [],\n",
       " ['is', 'going', 'snow'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['reports', '|'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['jared'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['related'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['expected'],\n",
       " ['expected'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['illustrated'],\n",
       " ['illustrated'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['windows', 'updated'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['addresses'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['settle'],\n",
       " ['is', 'has'],\n",
       " ['are'],\n",
       " ['is'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['hudgens'],\n",
       " [],\n",
       " [],\n",
       " ['-do-i-reach-out-or-step-back/article4178673/'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['talk'],\n",
       " [],\n",
       " ['ohio'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['massachusetts'],\n",
       " ['sperling'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['gretchen'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['is', 'megyn'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['girlfriend'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['happened'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['are'],\n",
       " ['see'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['clinton'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['sparking'],\n",
       " ['did', 'elon', 'go'],\n",
       " ['worry', 'have', \"'s\"],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['qualified'],\n",
       " [],\n",
       " ['uses'],\n",
       " [],\n",
       " ['know'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['uses'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['win', 'fivethirtyeight'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['uses'],\n",
       " ['–'],\n",
       " [],\n",
       " ['uses'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['–'],\n",
       " ['–'],\n",
       " [],\n",
       " ['–'],\n",
       " ['–'],\n",
       " [],\n",
       " ['uses'],\n",
       " [],\n",
       " [],\n",
       " ['uses'],\n",
       " ['–'],\n",
       " ['–'],\n",
       " ['–'],\n",
       " ['is'],\n",
       " ['know'],\n",
       " ['–'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['–'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['mph'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['have'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['do', 'look'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['writes'],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_verbs(tokenize(df_news))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb_health_queries_pop_list = ['do-have', 'get','causes', 'treat', 'do-need', 'help', 'apply', 'understanding', 'testing', 'does-take', 'taking']\n",
    "# adj_health_queries_pop_list = ['medical', 'dental', 'free', 'infectious', 'mental', 'common', 'american', 'many', 'pregnant', 'strong', 'best', 'much']\n",
    "# noun_health_queries_pop_list = ['university-rochester', 'side-effects', 'heat-transfer', 'disease-covid-19', 'cdc', 'cvs', 'covid-19-situation', 'medical-school', 'rochester-medical', 'service-uhs', 'coronavirus-update', 'situation-dashboard', 'type-diabetes', 'medical-center', 'university-health', 'developmental-behavioral', 'coronavirus-symptoms', 'health-service', 'causes-tratment', 'mayo-clinic']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health['datetime']= pd.to_datetime(df_health['datetime'])\n",
    "D = {\n",
    "    'health': 'health',\n",
    "    'care': 'care',\n",
    "    'effects': 'effects',\n",
    "    'effect': 'effects',\n",
    "    'diseas': 'disease',\n",
    "    'disease': 'disease',\n",
    "    'cdc': 'cdc',\n",
    "    'cv': 'cvs',\n",
    "    'cvs': 'cvs',\n",
    "    'medicin': 'medicine',\n",
    "    'medicine': 'medicine',\n",
    "    'covid': 'covid',\n",
    "    'test': 'test',\n",
    "    'doctor': 'doctor',\n",
    "    'center': 'center',\n",
    "    'pain': 'pain',\n",
    "    'sex': 'sex',\n",
    "    'caus': 'causes',\n",
    "    'causes': 'causes',\n",
    "    'coronaviru': 'coronavirus',\n",
    "    'coronavirus': 'coronavirus'\n",
    "}\n",
    "\n",
    "d_noun = {\n",
    "    'health': 0,\n",
    "    'care': 1,\n",
    "    'effects': 2,\n",
    "    'disease': 3,\n",
    "    'cdc': 4,\n",
    "    'cvs': 5,\n",
    "    'medicine': 6,\n",
    "    'covid': 7,\n",
    "    'test': 8,\n",
    "    'doctor': 9,\n",
    "    'center': 10,\n",
    "    'pain': 11,\n",
    "    'sex': 12,\n",
    "    'causes': 13,\n",
    "    'coronavirus': 14\n",
    "}\n",
    "\n",
    "\n",
    "d_noun_2 = {\n",
    "    0: 'health',\n",
    "    1: 'care',\n",
    "    2: 'effects',\n",
    "    3: 'disease',\n",
    "    4: 'cdc',\n",
    "    5: 'cvs',\n",
    "    6: 'medicine',\n",
    "    7:'covid',\n",
    "    8: 'test',\n",
    "    9: 'doctor',\n",
    "    10: 'center',\n",
    "    11: 'pain',\n",
    "    12: 'sex',\n",
    "    13: 'causes',\n",
    "    14: 'coronavirus'\n",
    "}\n",
    "\n",
    "history = []\n",
    "weekly_totals = []\n",
    "labels = [] \n",
    "for week, wdf in df_health.set_index('datetime').groupby(pd.Grouper(freq='W')):\n",
    "    if len(wdf) >0:\n",
    "        wdist = [0 for i in range(15)]\n",
    "        week_label = str(week).split(' ')[0]\n",
    "        labels.append(week_label)\n",
    "        weekly_totals.append(len(wdf))\n",
    "\n",
    "        weekly_queries = [str(q).lower() for q in wdf['query'].values.tolist()]\n",
    "        for q in weekly_queries:\n",
    "            tag = nltk.pos_tag_sents(q)\n",
    "            get_nouns(tag)\n",
    "            for n in noun_list:\n",
    "                for w in n:\n",
    "                    if w in D: \n",
    "                        wdist[d_noun[D[w]]]+=1\n",
    "                    elif ps.stem(w) in D:\n",
    "                        wdist[d_noun[D[ps.stem(w)]]]+=1\n",
    "        history.append(wdist)\n",
    "\n",
    "history = np.array(history)\n",
    "norm_hist = np.zeros((len(history), len(history[0])))\n",
    "for i in range(len(history)):\n",
    "    factor = weekly_totals[i]\n",
    "    for j in range(len(history[0])):\n",
    "        norm_hist[i][j] = float(history[i][j])/float(factor)\n",
    "#         print(i,j,norm_hist[i][j],factor)\n",
    "\n",
    "\n",
    "# print(history)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in range(history.shape[1]):\n",
    "    name = d_noun_2[col]\n",
    "    y = norm_hist[:,col]\n",
    "    plt.plot(range(len(y)),y,label=name)\n",
    "\n",
    "plt.xticks(range(len(labels)),labels, rotation=90)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Percent Searched')\n",
    "plt.title(\"Percentages of Key Search Terms By Week\")\n",
    "\n",
    "\n",
    "# To do:\n",
    "#     change dictionary to include stemmed versions\n",
    "#     just covid stuff\n",
    "#     plot with all\n",
    "# sync with Anis\n",
    "# then plot from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health['datetime']= pd.to_datetime(df_health['datetime'])\n",
    "D = {\n",
    "    'health': 'health',\n",
    "    'effects': 'effects',\n",
    "    'effect': 'effects',\n",
    "    'diseas': 'disease',\n",
    "    'disease': 'disease',\n",
    "    'cdc': 'cdc',\n",
    "    'medicin': 'medicine',\n",
    "    'medicine': 'medicine',\n",
    "    'covid': 'covid',\n",
    "    'test': 'test',\n",
    "    'doctor': 'doctor',\n",
    "    'caus': 'causes',\n",
    "    'causes': 'causes',\n",
    "    'coronaviru': 'coronavirus',\n",
    "    'coronavirus': 'coronavirus'\n",
    "}\n",
    "\n",
    "d_covid = {\n",
    "    'health': 0,\n",
    "    'effects': 1,\n",
    "    'disease': 2,\n",
    "    'cdc': 3,\n",
    "    'medicine': 4,\n",
    "    'covid': 5,\n",
    "    'test': 6,\n",
    "    'doctor': 7,\n",
    "    'causes': 8,\n",
    "    'coronavirus': 9\n",
    "}\n",
    "\n",
    "\n",
    "d_covid_2 = {\n",
    "    0: 'health',\n",
    "    1: 'effects',\n",
    "    2: 'disease',\n",
    "    3: 'cdc',\n",
    "    4: 'medicine',\n",
    "    5:'covid',\n",
    "    6: 'test',\n",
    "    7: 'doctor',\n",
    "    8: 'causes',\n",
    "    9: 'coronavirus'\n",
    "}\n",
    "\n",
    "history = []\n",
    "weekly_totals = []\n",
    "labels = [] \n",
    "for week, wdf in df_health.set_index('datetime').groupby(pd.Grouper(freq='W')):\n",
    "    if len(wdf) >0:\n",
    "        wdist = [0 for i in range(10)]\n",
    "        week_label = str(week).split(' ')[0]\n",
    "        labels.append(week_label)\n",
    "        weekly_totals.append(len(wdf))\n",
    "\n",
    "        weekly_queries = [str(q).lower() for q in wdf['query'].values.tolist()]\n",
    "        for q in weekly_queries:\n",
    "            tag = nltk.pos_tag_sents(q)\n",
    "            get_nouns(tag)\n",
    "            for n in noun_list:\n",
    "                for w in n:\n",
    "                    if w in D: \n",
    "                        wdist[d_covid[D[w]]]+=1\n",
    "                    elif ps.stem(w) in D:\n",
    "                        wdist[d_covid[D[ps.stem(w)]]]+=1\n",
    "        history.append(wdist)\n",
    "\n",
    "history = np.array(history)\n",
    "norm_hist = np.zeros((len(history), len(history[0])))\n",
    "for i in range(len(history)):\n",
    "    factor = weekly_totals[i]\n",
    "    for j in range(len(history[0])):\n",
    "        norm_hist[i][j] = float(history[i][j])/float(factor)\n",
    "#         print(i,j,norm_hist[i][j],factor)\n",
    "\n",
    "\n",
    "# print(history)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in range(history.shape[1]):\n",
    "    name = d_covid_2[col]\n",
    "    y = norm_hist[:,col]\n",
    "    plt.plot(range(len(y)),y,label=name)\n",
    "\n",
    "plt.xticks(range(len(labels)),labels, rotation=90)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Percent Searched')\n",
    "plt.title(\"Percentages of Key Search Terms By Week\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health['datetime']= pd.to_datetime(df_health['datetime'])\n",
    "D = {\n",
    "    'medic':'medical',\n",
    "    'medical': 'medical',\n",
    "    'best':'best',\n",
    "    'much':'much',\n",
    "    'rid': 'rid',\n",
    "    'mani': 'many',\n",
    "    'many': 'many',\n",
    "    'public':'public',\n",
    "    'earli' : 'early',\n",
    "    'early':'early',\n",
    "    'free': 'free',\n",
    "    'more': 'more',\n",
    "    'human': 'human',\n",
    "    'long':'long',\n",
    "    'pregnant':'pregnant'\n",
    "}\n",
    "\n",
    "d_adj = {\n",
    "    'medical': 0,\n",
    "    'best': 1,\n",
    "    'much': 2,\n",
    "    'rid': 3,\n",
    "    'many': 4,\n",
    "    'public': 5,\n",
    "    'early': 6,\n",
    "    'free': 7,\n",
    "    'more': 8,\n",
    "    'human': 9,\n",
    "    'long': 10,\n",
    "    'pregnant': 11\n",
    "}\n",
    "\n",
    "d_adj_2 = {\n",
    "    0: 'medical',\n",
    "    1: 'best',\n",
    "    2: 'much',\n",
    "    3: 'rid',\n",
    "    4: 'many',\n",
    "    5: 'public',\n",
    "    6: 'early',\n",
    "    7: 'free',\n",
    "    8: 'more',\n",
    "    9: 'human',\n",
    "    10: 'long',\n",
    "    11: 'pregnant'\n",
    "    \n",
    "}\n",
    "\n",
    "history = []\n",
    "weekly_totals = []\n",
    "labels = [] \n",
    "for week, wdf in df_health.set_index('datetime').groupby(pd.Grouper(freq='W')):\n",
    "    if len(wdf) >0:\n",
    "        wdist = [0 for i in range(12)]\n",
    "        week_label = str(week).split(' ')[0]\n",
    "        labels.append(week_label)\n",
    "        weekly_totals.append(len(wdf))\n",
    "#         print(week_label, len(wdf))\n",
    "        weekly_queries = [str(q).lower() for q in wdf['query'].values.tolist()]\n",
    "        for q in weekly_queries:\n",
    "            tag = nltk.pos_tag_sents(q)\n",
    "            get_adj(tag)\n",
    "            for n in adj_list:\n",
    "                for w in n:\n",
    "                    if w in D: \n",
    "                        wdist[d_adj[D[w]]]+=1\n",
    "                    elif ps.stem(w) in D:\n",
    "                        wdist[d_adj[D[ps.stem(w)]]]+=1\n",
    "        history.append(wdist)\n",
    "# print(weekly_totals)\n",
    "history = np.array(history)\n",
    "# print(history)\n",
    "norm_hist = np.zeros((len(history), len(history[0])))\n",
    "for i in range(len(history)):\n",
    "    factor = weekly_totals[i]\n",
    "    for j in range(len(history[0])):\n",
    "        norm_hist[i][j] = float(history[i][j])/float(factor)\n",
    "#         print(i,j,norm_hist[i][j],factor)\n",
    "\n",
    "\n",
    "# print(history)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in range(history.shape[1]):\n",
    "    name = d_adj_2[col]\n",
    "    y = norm_hist[:,col]\n",
    "    plt.plot(range(len(y)),y,label=name)\n",
    "\n",
    "plt.xticks(range(len(labels)),labels, rotation=90)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Percent Searched')\n",
    "plt.title(\"Percentages of Key Search Terms By Week\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health['datetime']= pd.to_datetime(df_health['datetime'])\n",
    "D = {\n",
    "    'treat':'treat',\n",
    "    'need':'need',\n",
    "    'get':'get',\n",
    "    'caus': 'causes',\n",
    "    'causes': 'causes',\n",
    "    'help': 'help',\n",
    "    'take': 'take',\n",
    "    'know':'know',\n",
    "    'explain': 'explained',\n",
    "    'explained': 'explained'\n",
    "}\n",
    "\n",
    "d_verb = {\n",
    "    'treat': 0,\n",
    "    'need': 1,\n",
    "    'get': 2,\n",
    "    'causes': 3,\n",
    "    'help': 4,\n",
    "    'take': 5,\n",
    "    'know': 6,\n",
    "    'explained': 7\n",
    "}\n",
    "\n",
    "d_verb_2 = {\n",
    "    0: 'treat',\n",
    "    1: 'need',\n",
    "    2: 'get',\n",
    "    3: 'causes',\n",
    "    4: 'help',\n",
    "    5: 'take',\n",
    "    6: 'know',\n",
    "    7: 'explained'\n",
    "}\n",
    "\n",
    "history = []\n",
    "weekly_totals = []\n",
    "labels = [] \n",
    "for week, wdf in df_health.set_index('datetime').groupby(pd.Grouper(freq='W')):\n",
    "    if len(wdf) >0:\n",
    "        wdist = [0 for i in range(8)]\n",
    "        week_label = str(week).split(' ')[0]\n",
    "        labels.append(week_label)\n",
    "        weekly_totals.append(len(wdf))\n",
    "#         print(week_label, len(wdf))\n",
    "        weekly_queries = [str(q).lower() for q in wdf['query'].values.tolist()]\n",
    "        for q in weekly_queries:\n",
    "            tag = nltk.pos_tag_sents(q)\n",
    "            get_verbs(tag)\n",
    "            for n in verb_list:\n",
    "                for w in n:\n",
    "                    if w in D: \n",
    "                        wdist[d_verb[D[w]]]+=1\n",
    "                    elif ps.stem(w) in D:\n",
    "                        wdist[d_verb[D[wps.stem(w)]]]+=1\n",
    "        history.append(wdist)\n",
    "# print(weekly_totals)\n",
    "history = np.array(history)\n",
    "# print(history)\n",
    "norm_hist = np.zeros((len(history), len(history[0])))\n",
    "for i in range(len(history)):\n",
    "    factor = weekly_totals[i]\n",
    "    for j in range(len(history[0])):\n",
    "        norm_hist[i][j] = float(history[i][j])/float(factor)\n",
    "#         print(i,j,norm_hist[i][j],factor)\n",
    "\n",
    "\n",
    "# print(history)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in range(history.shape[1]):\n",
    "    name = d_verb_2[col]\n",
    "    y = norm_hist[:,col]\n",
    "    plt.plot(range(len(y)),y,label=name)\n",
    "\n",
    "plt.xticks(range(len(labels)),labels, rotation=90)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Percent Searched')\n",
    "plt.title(\"Percentages of Key Search Terms By Week\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-390-6342398c1ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                         \u001b[0mwdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_school\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     \u001b[0;32melif\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                         \u001b[0mwdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_school\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/covidProject/lib/python3.7/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/covidProject/lib/python3.7/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step2\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"iveness\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"fulness\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ful\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"ousness\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ous\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"aliti\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"al\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"iviti\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_health['datetime']= pd.to_datetime(df_health['datetime'])\n",
    "\n",
    "D = {\n",
    "    'research':'research',\n",
    "    'mcat': 'mcat',\n",
    "    'school': 'school',\n",
    "    'rochest': 'rochester',\n",
    "    'rochester': 'rochester'\n",
    "}\n",
    "\n",
    "d_school = {\n",
    "    'research': 0,\n",
    "    'mcat': 1,\n",
    "    'school': 2,\n",
    "    'rochester': 3\n",
    "}\n",
    "\n",
    "d_school_2 = {\n",
    "    0: 'research',\n",
    "    1: 'mcat',\n",
    "    2: 'school',\n",
    "    3: 'rochester'\n",
    "}\n",
    "\n",
    "history = []\n",
    "weekly_totals = []\n",
    "labels = [] \n",
    "for week, wdf in df_health.set_index('datetime').groupby(pd.Grouper(freq='W')):\n",
    "    if len(wdf) >0:\n",
    "        wdist = [0 for i in range(4)]\n",
    "        week_label = str(week).split(' ')[0]\n",
    "        labels.append(week_label)\n",
    "        weekly_totals.append(len(wdf))\n",
    "#         print(week_label, len(wdf))\n",
    "        weekly_queries = [str(q).lower() for q in wdf['query'].values.tolist()]\n",
    "        for q in weekly_queries:\n",
    "            tag = nltk.pos_tag_sents(q)\n",
    "            get_nouns(tag)\n",
    "            for n in noun_list:\n",
    "                for w in n:\n",
    "                    if w in D: \n",
    "                        wdist[d_school[D[w]]]+=1\n",
    "                    elif ps.stem(w) in D:\n",
    "                        wdist[d_school[D[ps.stem(w)]]]+=1\n",
    "        history.append(wdist)\n",
    "# print(weekly_totals)\n",
    "history = np.array(history)\n",
    "# print(history)\n",
    "norm_hist = np.zeros((len(history), len(history[0])))\n",
    "for i in range(len(history)):\n",
    "    factor = weekly_totals[i]\n",
    "    for j in range(len(history[0])):\n",
    "        norm_hist[i][j] = float(history[i][j])/float(factor)\n",
    "#         print(i,j,norm_hist[i][j],factor)\n",
    "\n",
    "\n",
    "# print(history)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in range(history.shape[1]):\n",
    "    name = d_school_2[col]\n",
    "    y = norm_hist[:,col]\n",
    "    plt.plot(range(len(y)),y,label=name)\n",
    "\n",
    "plt.xticks(range(len(labels)),labels, rotation=90)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Percent Searched')\n",
    "plt.title(\"Percentages of Key Search Terms By Week\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def bigram(list_name): \n",
    "#     string = \"\"\n",
    "#     for l in list_name:\n",
    "#         if len(l) > 1:\n",
    "#             for word in l:\n",
    "#                 string = string + \" \" + word\n",
    "#             tokens = nltk.word_tokenize(string)  \n",
    "#             bigrm = nltk.bigrams(tokens) \n",
    "#             print(*map('-'.join, bigrm), sep=', ')\n",
    "#         else:\n",
    "#             print(l)\n",
    "#         string = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "democraci\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "\n",
    "print(ps.stem('democracy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['datetime']= pd.to_datetime(df_news['datetime'])\n",
    "D = {\n",
    "    'economy': 'economy',\n",
    "    'economi': 'economy',\n",
    "    'polit':'politics',\n",
    "    'politics': 'politics',\n",
    "    'trump': 'trump',\n",
    "    'weather': 'weather',\n",
    "    'news': 'news',\n",
    "    'coronavirus': 'coronavirus',\n",
    "    'coronaviru': 'coronavirus',\n",
    "    'worth': 'worth',\n",
    "    'york': 'york',\n",
    "    'journal': 'journal',\n",
    "    'check': 'check',\n",
    "    'debate': 'debate',\n",
    "    'debat':'debate',\n",
    "    'democracy': 'democracy',\n",
    "    'democraci': 'democracy'\n",
    "}\n",
    "\n",
    "d_news_noun = {\n",
    "    'economy': 0,\n",
    "    'politics': 1,\n",
    "    'trump': 2,\n",
    "    'weather': 3,\n",
    "    'news': 4,\n",
    "    'coronavirus': 5,\n",
    "    'worth': 6,\n",
    "    'york': 7,\n",
    "    'journal': 8,\n",
    "    'check': 9,\n",
    "    'debate': 10,\n",
    "    'democracy': 11\n",
    "}\n",
    "\n",
    "\n",
    "d_news_noun_2 = {\n",
    "    0: 'economy',\n",
    "    1: 'politics',\n",
    "    2: 'trump',\n",
    "    3: 'weather',\n",
    "    4: 'news',\n",
    "    5: 'coronavirus',\n",
    "    6: 'worth',\n",
    "    7:'york',\n",
    "    8: 'journal',\n",
    "    9: 'check',\n",
    "    10: 'debate',\n",
    "    11: 'democracy'\n",
    "}\n",
    "\n",
    "history = []\n",
    "weekly_totals = []\n",
    "labels = [] \n",
    "for week, wdf in df_news.set_index('datetime').groupby(pd.Grouper(freq='W')):\n",
    "    if len(wdf) >0:\n",
    "        wdist = [0 for i in range(17)]\n",
    "        week_label = str(week).split(' ')[0]\n",
    "        labels.append(week_label)\n",
    "        weekly_totals.append(len(wdf))\n",
    "\n",
    "        weekly_queries = [str(q).lower() for q in wdf['query'].values.tolist()]\n",
    "        for q in weekly_queries:\n",
    "            tagged_list = nltk.pos_tag_sents(q)\n",
    "            get_nouns(tagged_list)\n",
    "            for n in noun_list:\n",
    "                for w in n:\n",
    "                    if w in D: \n",
    "                        wdist[d_news_noun[D[w]]]+=1\n",
    "                    elif ps.stem(w) in D:\n",
    "                        wdist[d_news_noun[D[ps.stem(w)]]]+=1\n",
    "        history.append(wdist)\n",
    "\n",
    "history = np.array(history)\n",
    "norm_hist = np.zeros((len(history), len(history[0])))\n",
    "for i in range(len(history)):\n",
    "    factor = weekly_totals[i]\n",
    "    for j in range(len(history[0])):\n",
    "        norm_hist[i][j] = float(history[i][j])/float(factor)\n",
    "#         print(i,j,norm_hist[i][j],factor)\n",
    "\n",
    "\n",
    "# print(history)\n",
    "plt.figure(figsize=(12, 8))\n",
    "for col in range(history.shape[1]):\n",
    "    name = d_news_noun_2[col]\n",
    "    y = norm_hist[:,col]\n",
    "    plt.plot(range(len(y)),y,label=name)\n",
    "\n",
    "plt.xticks(range(len(labels)),labels, rotation=90)\n",
    "plt.legend(fontsize = 12)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Percent Searched')\n",
    "plt.title(\"Percentages of Key Search Terms By Week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
